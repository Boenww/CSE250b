{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "The large number of English words can make language-based applications daunting. To cope with this, it is helpful to have a clustering or embedding of these words, so that words with similar meanings are clustered together, or have embeddings that are close to one another. That is, words that tend to appear in similar contexts are likely to be related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "#nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pr(c|w) & Pr(c)\n",
    "\n",
    "1. Remove stopwords and punctuation, make everything lowercase, and count how often each word occurs.\n",
    "Use this to come up with two lists:\n",
    "\\begin{itemize}\n",
    "\\item A vocabulary V , consisting of a few thousand (e.g., 5000) of the most commonly-occurring words.\n",
    "\\item A shorter list C of at most 1000 of the most commonly-occurring words, which we shall call\n",
    "context words.\n",
    "\\end{itemize}\n",
    "   \n",
    "2. For each word $w \\in V$, and each occurrence of it in the text stream, look at the surrounding window of\n",
    "four words (two before, two after): $w_1, w_2, w, w_3, w_4$. Keep count of how often context words from C appear in these positions around word w. That is, for\n",
    "$w \\in V$, $c \\in C$, define:\n",
    "                   \n",
    "                   n(w, c) = # of times c occurs in a window around w\n",
    "Using these counts, construct the probability distribution Pr(c|w) of context words around w (for each $w \\in V$), as well as the overall distribution Pr(c) of context words. These are distributions over C.\n",
    "    \n",
    "3. Represent each vocabulary item w by a |C|-dimensional vector $\\Phi(w)$, whose c'th coordinate is:\n",
    "\\begin{equation*}\n",
    "\\Phi_c(w) = max(0, log\\dfrac{Pr(c|w)}{Pr(c)})\n",
    "\\end{equation*}\n",
    "This is known as the (positive) pointwise mutual information, and has been quite successful in work\n",
    "on word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bowen/miniconda2/lib/python2.7/site-packages/ipykernel_launcher.py:50: RuntimeWarning: divide by zero encountered in log2\n"
     ]
    }
   ],
   "source": [
    "words = nltk.corpus.brown.words()\n",
    "filteredWords = []\n",
    "wordCount = defaultdict(int)\n",
    "\n",
    "for word in words:\n",
    "    if word.lower() not in set(stopwords.words('english')):\n",
    "        w = ''.join([c for c in word.lower() if c not in set(punctuation)])\n",
    "        if w != '':\n",
    "            wordCount[w] += 1\n",
    "            filteredWords.append(w)\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "V = [count[1] for count in counts[:5000]] \n",
    "C = [count[1] for count in counts[:1000]]\n",
    "\n",
    "#Pr(c)\n",
    "Pr_c = defaultdict(float)\n",
    "for c in C:\n",
    "    Pr_c[c] = wordCount[c] * 1.0 / sum(wordCount[c] for c in C)\n",
    "    \n",
    "wWindow = defaultdict(list)\n",
    "for i in xrange(0, len(filteredWords)):\n",
    "    if filteredWords[i] in V:\n",
    "        if i == 0:\n",
    "            wWindow[filteredWords[i]].extend(neigh for neigh in [filteredWords[i+1], filteredWords[i+2], filteredWords[i+3], filteredWords[i+4]] if neigh in C)\n",
    "        if i == 1:\n",
    "            wWindow[filteredWords[i]].extend(neigh for neigh in [filteredWords[i-1], filteredWords[i+1], filteredWords[i+2], filteredWords[i+3]] if neigh in C)\n",
    "        if 2 <= i <= len(filteredWords) - 3:\n",
    "            wWindow[filteredWords[i]].extend(neigh for neigh in [filteredWords[i-2], filteredWords[i-1], filteredWords[i+1], filteredWords[i+2]] if neigh in C)\n",
    "        if i == len(filteredWords) - 2:\n",
    "            wWindow[filteredWords[i]].extend(neigh for neigh in [filteredWords[i+1], filteredWords[i-1], filteredWords[i-2], filteredWords[i-3]] if neigh in C)\n",
    "        if i == len(filteredWords) - 1:\n",
    "            wWindow[filteredWords[i]].extend(neigh for neigh in [filteredWords[i-1], filteredWords[i-2], filteredWords[i-3], filteredWords[i-4]] if neigh in C)\n",
    "\n",
    "#Pr(c|w)\n",
    "nwc = defaultdict(int)\n",
    "for w in V:\n",
    "    for c in C:\n",
    "        nwc[(w, c)] = wWindow[w].count(c)\n",
    "        #Pr_cw[i, j] = nwc[(V[i], C[j])] * 1.0 / len(wWindow[V[i]])\n",
    "        \n",
    "def Pr_cw(c, w):\n",
    "    return nwc[(w, c)] * 1.0 / len(wWindow[w]) #Pr_c[c]\n",
    "\n",
    "phi_c = defaultdict(list)\n",
    "for w in V:\n",
    "    phi_c[w] = [max(0, np.log2(Pr_cw(c, w) * 1.0 / Pr_c[c])) for c in C]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "a 100-dimensional representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k, v = list(zip(*phi_c.items()))\n",
    "pca = PCA(n_components=100,svd_solver='full') #auto -> randomized\n",
    "v_rd = pca.fit_transform(v)\n",
    "phi_c_rd = dict(zip(k, v_rd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN\n",
    "\n",
    "Pick a collection of 25 words $w \\in V$. For each w, return its nearest neighbor $w'\\neq w$ in V . A popular distance measure to use for this is cosine distance:\n",
    "\\begin{equation*}\n",
    "1 - \\dfrac{\\Psi(w)\\cdot\\Psi(w')}{||\\Psi(w)||\\cdot||\\Psi(w')||}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(wa, wb):\n",
    "    return 1 - phi_c_rd[wa].dot(phi_c_rd[wb]) * 1.0 / (np.linalg.norm(phi_c_rd[wa]) * np.linalg.norm(phi_c_rd[wb]))\n",
    "\n",
    "def nn(word, Vacabulary):\n",
    "    allDistance = []\n",
    "    for w in Vacabulary:\n",
    "        if w != word:\n",
    "            allDistance.append(distance(word, w)) \n",
    "    if np.argmin(allDistance) >= Vacabulary.index(word):\n",
    "        return Vacabulary[np.argmin(allDistance)+1]\n",
    "    else:\n",
    "        return Vacabulary[np.argmin(allDistance)]\n",
    "\n",
    "# nn = NearestNeighbors(n_neighbors=2, metric='cosine')\n",
    "# nn.fit(V)\n",
    "# nn.kneighbors(phi_c_rd['communism'].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'utopian', u'summer', u'bullet', u'artery', u'world', u'asia', u'portland', u'intellectual', u'july', u'drugs', u'fabrics', u'text', u'saturday', u'religious', u'sunday', u'courts', u'committee', u'liberals', u'useful', u'measuring', u'policeman', u'government', u'15', u'problems', u'pay']\n"
     ]
    }
   ],
   "source": [
    "wordC = ['communism', 'autumn', 'cigarette', 'pulmonary', 'mankind', 'africa', 'chicago', 'revolution', 'september', 'chemical', 'detergent', 'dictionary', 'storm', 'worship', 'friday', 'jury', 'election', 'widespread', 'considering', 'size', 'excuse', 'governments', 'date', 'major', 'money']\n",
    "nNeighs = []\n",
    "\n",
    "for word in wordC:\n",
    "    nNeighs.append(nn(word, k))\n",
    "\n",
    "print nNeighs\n",
    "\n",
    "# NN = NearestNeighbors(n_neighbors=2, metric='cosine')\n",
    "# NN.fit(V)\n",
    "# NN.kneighbors(phi_c_rd['communism'].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nearest neighbors of some words are semantically related with them\n",
    "or belong to the same category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "Using the vectorial representation $\\Psi(\\cdot)$, cluster the words in V into 100 groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = GaussianMixture(n_components=100)\n",
    "gm.fit(v_rd)\n",
    "groups = gm.predict(v_rd)\n",
    "\n",
    "clusters = defaultdict(list)\n",
    "for i in xrange(0, len(groups)):\n",
    "    clusters[groups[i]].append(k[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'saturday', u'pm', u'am', u'tuesday'],\n",
       " [u'standard',\n",
       "  u'directly',\n",
       "  u'provided',\n",
       "  u'prices',\n",
       "  u'apply',\n",
       "  u'institution',\n",
       "  u'sources'],\n",
       " [u'henry',\n",
       "  u'john',\n",
       "  u'george',\n",
       "  u'mrs',\n",
       "  u'director',\n",
       "  u'richard',\n",
       "  u'robert',\n",
       "  u'thomas',\n",
       "  u'william'],\n",
       " [u'street', u'side', u'across', u'road', u'river'],\n",
       " [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8'],\n",
       " [u'memorial',\n",
       "  u'mayor',\n",
       "  u'miami',\n",
       "  u'st',\n",
       "  u'louis',\n",
       "  u'author',\n",
       "  u'greenwich',\n",
       "  u'governor',\n",
       "  u'chairman',\n",
       "  u'maryland',\n",
       "  u'houston',\n",
       "  u'atlanta',\n",
       "  u'francisco'],\n",
       " [u'direct', u'method', u'laws', u'described', u'analysis']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smallClusters = []\n",
    "for i in xrange(0, len(clusters)):\n",
    "    if len(clusters[i]) <= 15:\n",
    "        smallClusters.append(clusters[i])\n",
    "        \n",
    "smallClusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'todays', u'standards'],\n",
       " [u'india',\n",
       "  u'entitled',\n",
       "  u'shall',\n",
       "  u'commission',\n",
       "  u'commerce',\n",
       "  u'legislation',\n",
       "  u'title',\n",
       "  u'act',\n",
       "  u'directed',\n",
       "  u'authorized',\n",
       "  u'sec',\n",
       "  u'provision',\n",
       "  u'provisions'],\n",
       " [u'values', u'behavior', u'value'],\n",
       " [u'daily', u'14', u'6', u'24', u'feed'],\n",
       " [u'l', u'n', u'p', u'r', u'b', u'c', u'e', u'f', u'g', u'j'],\n",
       " [u'division', u'uses', u'follows', u'requires', u'establishment'],\n",
       " [u'wednesday',\n",
       "  u'saturday',\n",
       "  u'reception',\n",
       "  u'sunday',\n",
       "  u'thursday',\n",
       "  u'friday',\n",
       "  u'scheduled',\n",
       "  u'yesterday',\n",
       "  u'am',\n",
       "  u'oclock',\n",
       "  u'tuesday',\n",
       "  u'monday']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km = KMeans(n_clusters=100).fit(v_rd)\n",
    "groupsKM = km.labels_\n",
    "\n",
    "clustersKM = defaultdict(list)\n",
    "for i in xrange(0, len(groupsKM)):\n",
    "    clustersKM[groupsKM[i]].append(k[i])\n",
    "\n",
    "smallClustersKM = []\n",
    "for i in xrange(0, len(clustersKM)):\n",
    "    if len(clustersKM[i]) <= 15:\n",
    "        smallClustersKM.append(clustersKM[i])\n",
    "        \n",
    "smallClustersKM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian Mixture model was selected to fit the data into 100 groups rather than K-Means, because\n",
    "besides of the means, the covariance information (important to this word embeddings clustering case)\n",
    "of the data is also utilized, which could generate more generalized clusters to some extent. The expectation-maximization algorithm is implemented in its learning process, which firstly initializes\n",
    "random components and repeatedly assigns each point a probability between the clusters and then\n",
    "updates the parameters including weights, means and covariances to maximize the likelihood given\n",
    "the assignments until convergence. The distance function involved in is the Mahalanobis distances\n",
    "between points and centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
